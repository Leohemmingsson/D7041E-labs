{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/leo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/leo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import unidecode as un\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('IMDB_Dataset.csv')\n",
    "df_old = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "We also need to make sure that the sentiments are only positive or negative (in case of faults in data)\n",
    "\n",
    "We also need to remove HTML tags, due to the reviews being scraped from IMDB\n",
    "\n",
    "We start by removing special letters etc. with unicode. This will change é to e á to a etc. \n",
    "\n",
    "After this we remove all special characters and make the comments clean. \n",
    "\n",
    "Removing stopwords\n",
    "\n",
    "Lemmatizing the data\n",
    "\n",
    "Finally we also remove unnecessary spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7560/1069883621.py:9: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  df[\"review\"] = df[\"review\"].apply(lambda x: BeautifulSoup(x, \"html.parser\").get_text())\n"
     ]
    }
   ],
   "source": [
    "# Remove if sentiment is not positive or negative\n",
    "mask = df['sentiment'].isin(['positive', 'negative'])\n",
    "df = df[mask]\n",
    "\n",
    "# Lablenize sentiment\n",
    "df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "\n",
    "# Remove html tags\n",
    "df[\"review\"] = df[\"review\"].apply(lambda x: BeautifulSoup(x, \"html.parser\").get_text())\n",
    "\n",
    "# Fix decode and allowed_chars\n",
    "allowed_chars = \" abcdefghijklmnopqrstuvwxyz0123456789\"\n",
    "df[\"review\"] = df[\"review\"].apply(lambda x: un.unidecode(x).lower())\n",
    "df[\"review\"] = df[\"review\"].apply(lambda x: ''.join([i for i in x if i in allowed_chars]))\n",
    "\n",
    "# Remove stopwords\n",
    "stop = stopwords.words('english')\n",
    "df[\"review\"] = df[\"review\"].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "\n",
    "# Lemmatize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df[\"review\"] = df[\"review\"].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))\n",
    "\n",
    "\n",
    "# Strip unnecessary spaces\n",
    "df[\"review\"] = df[\"review\"].apply(lambda x: x.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data\n",
    "The data will be split into training and testing data as well as labels and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 40000\n",
    "train_data, train_labels = df['review'][:train_size], df['sentiment'][:train_size]\n",
    "test_data, test_labels = df['review'][train_size:], df['sentiment'][train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ngram\n",
    "Create ngram representation of each word in the review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv_train_reviews.shape = (40000, 7144506)\n",
      "cv_test_reviews.shape = (10000, 7144506)\n"
     ]
    }
   ],
   "source": [
    "# ngram\n",
    "c = CountVectorizer(min_df=0.0, max_df=1.0, binary=False, ngram_range=(1,3))\n",
    "cv_train_reviews = c.fit_transform(train_data)\n",
    "cv_test_reviews = c.transform(test_data)\n",
    "\n",
    "print(f\"{cv_train_reviews.shape = }\")\n",
    "print(f\"{cv_test_reviews.shape = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
