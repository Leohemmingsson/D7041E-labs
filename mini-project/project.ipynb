{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/leo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/leo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import unidecode as un\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from bs4 import BeautifulSoup\n",
    "from custom_logistic_regression import OwnLogisticRegression\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('IMDB_Dataset.csv')\n",
    "df_old = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "We also need to make sure that the sentiments are only positive or negative (in case of faults in data)\n",
    "\n",
    "We also need to remove HTML tags, due to the reviews being scraped from IMDB\n",
    "\n",
    "We start by removing special letters etc. with unicode. This will change é to e á to a etc. \n",
    "\n",
    "After this we remove all special characters and make the comments clean. \n",
    "\n",
    "Removing stopwords\n",
    "\n",
    "Lemmatizing the data\n",
    "\n",
    "Finally we also remove unnecessary spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df):\n",
    "    # Remove if sentiment is not positive or negative\n",
    "    mask = df['sentiment'].isin(['positive', 'negative'])\n",
    "    df = df[mask]\n",
    "\n",
    "    # Lablenize sentiment\n",
    "    df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "\n",
    "    # Remove html tags\n",
    "    df[\"review\"] = df[\"review\"].apply(lambda x: BeautifulSoup(x, \"html.parser\").get_text())\n",
    "\n",
    "    # Fix decode and allowed_chars\n",
    "    allowed_chars = \" abcdefghijklmnopqrstuvwxyz0123456789\"\n",
    "    df[\"review\"] = df[\"review\"].apply(lambda x: un.unidecode(x).lower())\n",
    "    df[\"review\"] = df[\"review\"].apply(lambda x: ''.join([i for i in x if i in allowed_chars]))\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop = stopwords.words('english')\n",
    "    df[\"review\"] = df[\"review\"].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "\n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    df[\"review\"] = df[\"review\"].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))\n",
    "\n",
    "\n",
    "    # Strip unnecessary spaces\n",
    "    df[\"review\"] = df[\"review\"].apply(lambda x: x.strip())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data\n",
    "The data will be split into training and testing data as well as labels and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(df, train_size=40000):\n",
    "    train_data, train_labels = df['review'][:train_size], df['sentiment'][:train_size]\n",
    "    test_data, test_labels = list(df['review'][train_size:]), list(df['sentiment'][train_size:])\n",
    "    return (train_data, train_labels), (test_data, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "We start by using a regression model, this is imported from sklear, and does not need very much work or knowledge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39999/4183327440.py:10: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  df[\"review\"] = df[\"review\"].apply(lambda x: BeautifulSoup(x, \"html.parser\").get_text())\n"
     ]
    }
   ],
   "source": [
    "df = preprocessing(df)\n",
    "(train_data, train_labels), (test_data, test_labels) = split_train_test(df)\n",
    "\n",
    "# Ngram representation\n",
    "c = CountVectorizer(min_df=0.0, max_df=1.0, binary=False, ngram_range=(1,3))\n",
    "cv_train_reviews = c.fit_transform(train_data)\n",
    "cv_test_reviews = c.transform(test_data)\n",
    "\n",
    "lr = LogisticRegression(penalty=\"l2\", max_iter=500, C=1, random_state=42)\n",
    "lr_bow = lr.fit(cv_train_reviews, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training own written logistic regression\n",
    "# lr_own = OwnLogisticRegression()\n",
    "# lr_own.fit(cv_test_reviews.toarray(), train_labels, epochs=150)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_Bow_predict = lr.predict(cv_test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(answer:list , predicted:list):\n",
    "    \"\"\"\n",
    "    Compare each of the values in answer with predicted.\n",
    "    Returns the accuracy\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    for i in range(len(predicted)):\n",
    "        if answer[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct/len(predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.8998\n"
     ]
    }
   ],
   "source": [
    "lr_bow_score = accuracy_score(test_labels, list(lr_Bow_predict))\n",
    "print(f\"Score: {lr_bow_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
