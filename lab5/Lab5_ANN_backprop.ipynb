{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions of non-linear activations\n",
    "def f_sigmoid(X, deriv=False):\n",
    "    if not deriv:\n",
    "        return 1 / (1 + np.exp(-X))\n",
    "    else:\n",
    "        return f_sigmoid(X)*(1 - f_sigmoid(X))\n",
    "\n",
    "\n",
    "def f_softmax(X):\n",
    "    Z = np.sum(np.exp(X), axis=1)\n",
    "    Z = Z.reshape(Z.shape[0], 1)\n",
    "    return np.exp(X) / Z\n",
    "\n",
    "def f_relu(X, deriv=False):\n",
    "    if not deriv:\n",
    "        return X*(X>0)\n",
    "    else:\n",
    "        return 1*(X>0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exit_with_err(err_str):\n",
    "    print(sys.stderr, err_str)\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functionality of a single hidden layer\n",
    "class Layer:\n",
    "    def __init__(self, size, batch_size, is_input=False, is_output=False,\n",
    "                 activation=f_sigmoid):\n",
    "        self.is_input = is_input\n",
    "        self.is_output = is_output\n",
    "\n",
    "        # Z is the matrix that holds output values\n",
    "        self.Z = np.zeros((batch_size, size[0]))\n",
    "        # The activation function is an externally defined function (with a\n",
    "        # derivative) that is stored here\n",
    "        self.activation = activation\n",
    "\n",
    "        # W is the outgoing weight matrix for this layer\n",
    "        self.W = None\n",
    "        # S is the matrix that holds the inputs to this layer\n",
    "        self.S = None\n",
    "        # D is the matrix that holds the deltas for this layer\n",
    "        self.D = None\n",
    "        # Fp is the matrix that holds the derivatives of the activation function\n",
    "        self.Fp = None\n",
    "\n",
    "        if not is_input:\n",
    "            self.S = np.zeros((batch_size, size[0]))\n",
    "            self.D = np.zeros((batch_size, size[0]))\n",
    "\n",
    "        if not is_output:\n",
    "            self.W = np.random.normal(size=size, scale=1E-4)\n",
    "\n",
    "        if not is_input and not is_output:\n",
    "            self.Fp = np.zeros((size[0], batch_size))\n",
    "\n",
    "    def forward_propagate(self):\n",
    "        if self.is_input:\n",
    "            return self.Z.dot(self.W)\n",
    "\n",
    "        self.Z = self.activation(self.S)\n",
    "        if self.is_output:\n",
    "            return self.Z\n",
    "        else:\n",
    "            # For hidden layers, we add the bias values here\n",
    "            self.Z = np.append(self.Z, np.ones((self.Z.shape[0], 1)), axis=1)\n",
    "            self.Fp = self.activation(self.S, deriv=True).T\n",
    "            return self.Z.dot(self.W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron:\n",
    "    def __init__(self, layer_config, batch_size=100):\n",
    "        self.layers = []\n",
    "        self.num_layers = len(layer_config)\n",
    "        self.minibatch_size = batch_size\n",
    "\n",
    "        for i in range(self.num_layers-1):\n",
    "            if i == 0:\n",
    "                print (\"Initializing input layer with size {0}.\".format(layer_config[i]))\n",
    "                # Here, we add an additional unit at the input for the bias\n",
    "                # weight.\n",
    "                self.layers.append(Layer([layer_config[i]+1, layer_config[i+1]],\n",
    "                                         batch_size,\n",
    "                                         is_input=True))\n",
    "            else:\n",
    "                print (\"Initializing hidden layer with size {0}.\".format(layer_config[i]))\n",
    "                # Here we add an additional unit in the hidden layers for the\n",
    "                # bias weight.\n",
    "                self.layers.append(Layer([layer_config[i]+1, layer_config[i+1]],\n",
    "                                         batch_size,\n",
    "                                         activation=f_relu))\n",
    "\n",
    "        print (\"Initializing output layer with size {0}.\".format(layer_config[-1]))\n",
    "        self.layers.append(Layer([layer_config[-1], None],\n",
    "                                 batch_size,\n",
    "                                 is_output=True,\n",
    "                                 activation=f_softmax))\n",
    "        print (\"Done!\")\n",
    "\n",
    "    def forward_propagate(self, data):\n",
    "        # We need to be sure to add bias values to the input\n",
    "        self.layers[0].Z = np.append(data, np.ones((data.shape[0], 1)), axis=1)\n",
    "\n",
    "        for i in range(self.num_layers-1):\n",
    "            self.layers[i+1].S = self.layers[i].forward_propagate()\n",
    "        return self.layers[-1].forward_propagate()\n",
    "\n",
    "    def backpropagate(self, yhat, labels):\n",
    "        \n",
    "        # exit_with_err(\"FIND ME IN THE CODE, What is computed in the next line of code?\\n\")\n",
    "\n",
    "        self.layers[-1].D = (yhat - labels).T\n",
    "        for i in range(self.num_layers-2, 0, -1):\n",
    "            # We do not calculate deltas for the bias values\n",
    "            W_nobias = self.layers[i].W[0:-1, :]\n",
    "            \n",
    "            # exit_with_err(\"FIND ME IN THE CODE, What does this 'for' loop do?\\n\")\n",
    "            \n",
    "            \n",
    "            self.layers[i].D = W_nobias.dot(self.layers[i+1].D) * self.layers[i].Fp\n",
    "\n",
    "    def update_weights(self, eta):\n",
    "        for i in range(0, self.num_layers-1):\n",
    "            W_grad = -eta*(self.layers[i+1].D.dot(self.layers[i].Z)).T\n",
    "            self.layers[i].W += W_grad\n",
    "\n",
    "    def evaluate(self, train_data, train_labels, test_data, test_labels,\n",
    "                 num_epochs=70, eta=0.05, eval_train=False, eval_test=True):\n",
    "\n",
    "        N_train = len(train_labels)*len(train_labels[0])\n",
    "        N_test = len(test_labels)*len(test_labels[0])\n",
    "\n",
    "        print (\"Training for {0} epochs...\".format(num_epochs))\n",
    "        for t in range(0, num_epochs):\n",
    "            out_str = \"[{0:4d}] \".format(t)\n",
    "\n",
    "            for b_data, b_labels in zip(train_data, train_labels):\n",
    "                output = self.forward_propagate(b_data)\n",
    "                self.backpropagate(output, b_labels)\n",
    "                \n",
    "                # exit_with_err(\"FIND ME IN THE CODE, How does weight update is implemented? What is eta?\\n\")\n",
    "\n",
    "                self.update_weights(eta=eta)\n",
    "\n",
    "            if eval_train:\n",
    "                errs = 0\n",
    "                for b_data, b_labels in zip(train_data, train_labels):\n",
    "                    output = self.forward_propagate(b_data)\n",
    "                    yhat = np.argmax(output, axis=1)\n",
    "                    errs += np.sum(1-b_labels[np.arange(len(b_labels)), yhat])\n",
    "\n",
    "                out_str = (\"{0} Training error: {1:.5f}\".format(out_str,\n",
    "                                                           float(errs)/N_train))\n",
    "\n",
    "            if eval_test:\n",
    "                errs = 0\n",
    "                for b_data, b_labels in zip(test_data, test_labels):\n",
    "                    output = self.forward_propagate(b_data)\n",
    "                    yhat = np.argmax(output, axis=1)\n",
    "                    errs += np.sum(1-b_labels[np.arange(len(b_labels)), yhat])\n",
    "\n",
    "                out_str = (\"{0} Test error: {1:.5f}\").format(out_str,\n",
    "                                                       float(errs)/N_test)\n",
    "\n",
    "            print (out_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_bit_vector(labels, nbits):\n",
    "    bit_vector = np.zeros((labels.shape[0], nbits))\n",
    "    for i in range(labels.shape[0]):\n",
    "        bit_vector[i, labels[i]] = 1.0\n",
    "\n",
    "    return bit_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(data, labels, batch_size, create_bit_vector=False):\n",
    "    N = data.shape[0]\n",
    "    print (\"Batch size {0}, the number of examples {1}.\".format(batch_size,N))\n",
    "\n",
    "    if N % batch_size != 0:\n",
    "        print (\"Warning in create_minibatches(): Batch size {0} does not \" \\\n",
    "              \"evenly divide the number of examples {1}.\".format(batch_size,N))\n",
    "    chunked_data = []\n",
    "    chunked_labels = []\n",
    "    idx = 0\n",
    "    while idx + batch_size <= N:\n",
    "        chunked_data.append(data[idx:idx+batch_size, :])\n",
    "        if not create_bit_vector:\n",
    "            chunked_labels.append(labels[idx:idx+batch_size])\n",
    "        else:\n",
    "            bit_vector = label_to_bit_vector(labels[idx:idx+batch_size], 10)\n",
    "            chunked_labels.append(bit_vector)\n",
    "\n",
    "        idx += batch_size\n",
    "\n",
    "    return chunked_data, chunked_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_backprop(batch_size, Train_images, Train_labels, Valid_images, Valid_labels):\n",
    "    \n",
    "    print (\"Creating data...\")\n",
    "    batched_train_data, batched_train_labels = create_batches(Train_images, Train_labels,\n",
    "                                              batch_size,\n",
    "                                              create_bit_vector=True)\n",
    "    batched_valid_data, batched_valid_labels = create_batches(Valid_images, Valid_labels,\n",
    "                                              batch_size,\n",
    "                                              create_bit_vector=True)\n",
    "    print (\"Done!\")\n",
    "\n",
    "\n",
    "    return batched_train_data, batched_train_labels,  batched_valid_data, batched_valid_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "(Xtr, Ltr), (X_test, L_test)=mnist.load_data()\n",
    "\n",
    "Xtr = Xtr.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "Xtr = Xtr.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "Xtr /= 255\n",
    "X_test /= 255\n",
    "print(Xtr.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating data...\n",
      "Batch size 100, the number of examples 60000.\n",
      "Batch size 100, the number of examples 10000.\n",
      "Done!\n",
      "Initializing input layer with size 784.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing output layer with size 10.\n",
      "Done!\n",
      "Training for 70 epochs...\n",
      "[   0]  Training error: 0.89558 Test error: 0.89720\n",
      "[   1]  Training error: 0.89558 Test error: 0.89720\n",
      "[   2]  Training error: 0.89558 Test error: 0.89720\n",
      "[   3]  Training error: 0.89558 Test error: 0.89720\n",
      "[   4]  Training error: 0.89558 Test error: 0.89720\n",
      "[   5]  Training error: 0.89558 Test error: 0.89720\n",
      "[   6]  Training error: 0.89558 Test error: 0.89720\n",
      "[   7]  Training error: 0.89558 Test error: 0.89720\n",
      "[   8]  Training error: 0.89558 Test error: 0.89720\n",
      "[   9]  Training error: 0.89558 Test error: 0.89720\n",
      "[  10]  Training error: 0.89558 Test error: 0.89720\n",
      "[  11]  Training error: 0.89558 Test error: 0.89720\n",
      "[  12]  Training error: 0.89558 Test error: 0.89720\n",
      "[  13]  Training error: 0.89558 Test error: 0.89720\n",
      "[  14]  Training error: 0.89558 Test error: 0.89720\n",
      "[  15]  Training error: 0.89558 Test error: 0.89720\n",
      "[  16]  Training error: 0.34352 Test error: 0.33640\n",
      "[  17]  Training error: 0.07792 Test error: 0.07820\n",
      "[  18]  Training error: 0.04655 Test error: 0.05040\n",
      "[  19]  Training error: 0.03570 Test error: 0.04270\n",
      "[  20]  Training error: 0.02587 Test error: 0.03460\n",
      "[  21]  Training error: 0.02200 Test error: 0.03190\n",
      "[  22]  Training error: 0.01970 Test error: 0.02980\n",
      "[  23]  Training error: 0.01682 Test error: 0.02840\n",
      "[  24]  Training error: 0.01937 Test error: 0.03200\n",
      "[  25]  Training error: 0.01363 Test error: 0.02840\n",
      "[  26]  Training error: 0.01143 Test error: 0.02730\n",
      "[  27]  Training error: 0.01488 Test error: 0.03260\n",
      "[  28]  Training error: 0.01048 Test error: 0.02700\n",
      "[  29]  Training error: 0.00972 Test error: 0.02850\n",
      "[  30]  Training error: 0.00832 Test error: 0.02580\n",
      "[  31]  Training error: 0.01033 Test error: 0.02710\n",
      "[  32]  Training error: 0.00795 Test error: 0.02700\n",
      "[  33]  Training error: 0.00822 Test error: 0.02740\n",
      "[  34]  Training error: 0.00858 Test error: 0.02640\n",
      "[  35]  Training error: 0.00642 Test error: 0.02490\n",
      "[  36]  Training error: 0.00447 Test error: 0.02270\n",
      "[  37]  Training error: 0.01428 Test error: 0.03410\n",
      "[  38]  Training error: 0.00560 Test error: 0.02470\n",
      "[  39]  Training error: 0.00528 Test error: 0.02600\n",
      "[  40]  Training error: 0.00477 Test error: 0.02620\n",
      "[  41]  Training error: 0.00458 Test error: 0.02410\n",
      "[  42]  Training error: 0.00360 Test error: 0.02290\n",
      "[  43]  Training error: 0.00275 Test error: 0.02100\n",
      "[  44]  Training error: 0.00357 Test error: 0.02170\n",
      "[  45]  Training error: 0.00427 Test error: 0.02510\n",
      "[  46]  Training error: 0.00285 Test error: 0.02280\n",
      "[  47]  Training error: 0.00630 Test error: 0.02660\n",
      "[  48]  Training error: 0.00125 Test error: 0.02260\n",
      "[  49]  Training error: 0.00172 Test error: 0.02430\n",
      "[  50]  Training error: 0.00022 Test error: 0.02050\n",
      "[  51]  Training error: 0.00008 Test error: 0.02020\n",
      "[  52]  Training error: 0.00172 Test error: 0.02330\n",
      "[  53]  Training error: 0.00148 Test error: 0.02340\n",
      "[  54]  Training error: 0.00023 Test error: 0.02150\n",
      "[  55]  Training error: 0.00013 Test error: 0.02030\n",
      "[  56]  Training error: 0.00000 Test error: 0.02050\n",
      "[  57]  Training error: 0.00000 Test error: 0.02040\n",
      "[  58]  Training error: 0.00000 Test error: 0.02010\n",
      "[  59]  Training error: 0.00000 Test error: 0.02000\n",
      "[  60]  Training error: 0.00000 Test error: 0.01980\n",
      "[  61]  Training error: 0.00000 Test error: 0.01980\n",
      "[  62]  Training error: 0.00000 Test error: 0.01970\n",
      "[  63]  Training error: 0.00000 Test error: 0.01980\n",
      "[  64]  Training error: 0.00000 Test error: 0.01980\n",
      "[  65]  Training error: 0.00000 Test error: 0.01980\n",
      "[  66]  Training error: 0.00000 Test error: 0.01980\n",
      "[  67]  Training error: 0.00000 Test error: 0.01970\n",
      "[  68]  Training error: 0.00000 Test error: 0.01950\n",
      "[  69]  Training error: 0.00000 Test error: 0.01950\n",
      "Done:)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size=100;\n",
    "\n",
    "train_data, train_labels, valid_data, valid_labels=prepare_for_backprop(batch_size, Xtr, Ltr, X_test, L_test)\n",
    "\n",
    "mlp = MultiLayerPerceptron(layer_config=[784, 100, 100, 10], batch_size=batch_size)\n",
    "\n",
    "mlp.evaluate(train_data, train_labels, valid_data, valid_labels,\n",
    "             eval_train=True, eta=0.005)\n",
    "\n",
    "print(\"Done:)\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model, X, y):\n",
    "    yhat = model.forward_propagate(X)\n",
    "    yhat = np.argmax(yhat, axis=1)\n",
    "    acc = np.sum(yhat == y) / float(len(y))\n",
    "    print(\"Accuracy: {0:.5f}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.98050\n"
     ]
    }
   ],
   "source": [
    "accuracy(mlp, X_test, L_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy reports\n",
    "**sigmoid eta 0.5** 0.09740\\\n",
    "**sigmoid eta 0.05** 0.97310\\\n",
    "**sigmoid eta 0.005** 0.97430\\\n",
    "**ReLu eta 0.05** 0.09580\\\n",
    "**ReLu eta 0.005** 0.98050"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "### Sigmoid\n",
    "#### For 0.005 Learning Rate\n",
    "**Convergence**: The MLP converged more slowly towards the minimum of the loss function. This is because smaller steps are taken during the gradient descent process.\n",
    "\n",
    "**Precision**: While it takes longer, a smaller learning rate lead to more precise convergence. But it should be observed that the difference with 0.05 is *very* small\n",
    "\n",
    "\n",
    "#### For 0.5 Learning Rate\n",
    "**Convergence**: The model made larger updates to the weights. This can significantly sped up the convergence process.\n",
    "\n",
    "**Precision**: We got a much lower precision compared to when running 0.05\n",
    "### ReLu\n",
    "When running eta 0.05 we got a way to low accuracy, so we tried running 0.005 as well. This lead to us trying with eta 0.005, which gave the best result. As for the speed, ReLu was way faster compared to sigmoid, and the difference of eta was not that noticeable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
